{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan los datos que resultaron del proceso de preparación de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/gusta/OneDrive/Documentos/Tesis/Datos/Datos_prep.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seleccionan las variables que serán utilizadas como variables dependientes, estas variables son las que cuentan con un buen IV para ser considerados en el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_predictivas = ['CC','VFA','CC_GRAS','CC_MUSC','CC_HUES','CC_AGUA','PAS',\n",
    "                  'PAM','PESO','PAD','TALLA','GLU','IMC']\n",
    "X = data[var_predictivas] \n",
    "y = data['TARGET_TRI'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar mejor el rendimiento del modelo, es una buena estrategia dividir el conjunto de datos dos: un conjunto de entrenamiento (70%) y uno de prueba (30%). Sin embargo, dado que la muestra de datos no esta balanceada (17.53% de Eventos vs 82.47% de No-Eventos) se hará primero un submuestreo (undersampling) el cual consiste en eliminar aleatoreamente algunos registros pertenecientes a la clase de No-Eventos (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 1479, 1: 278})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 1112, 1: 278})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=1,sampling_strategy=0.25)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled, \n",
    "                                                    test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos supervisados para Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se llevará a cabo la construcción de los modelos supervisados de Machine Learning que calculen la probabilidad de que el paciente tenga su nivel de Triglicéridos por arriba de los 150 mg/dL para posteriormente hacer la clasificación en Evento y No-Evento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye un modelo de Árbol de Decisión usando Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el modelo\n",
    "param_dict_tr= dict(criterion=['gini','entropy'], splitter=['best','random'],\n",
    "                 max_depth=range(2,30), max_features=range(2,5),\n",
    "                 max_leaf_nodes=range(5,35), min_samples_leaf=[0.05])\n",
    "\n",
    "# Crear el objeto árbol de decisión para clasificación\n",
    "tr_clf = DecisionTreeClassifier()\n",
    "\n",
    "#Autotuning de los modelos\n",
    "grid_tr = GridSearchCV(cv=5, estimator=tr_clf, n_jobs=-1, \n",
    "                     param_grid=param_dict_tr ,scoring='roc_auc')\n",
    "grid_tr.fit(X_train,y_train)\n",
    "tr_clf = grid_tr.best_estimator_\n",
    "\n",
    "# Entrenar el árbol de decisión\n",
    "tr_clf = tr_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye un modelo de Random Forest usando Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el modelo\n",
    "param_dict_rf= dict(n_estimators=range(50,150), criterion=['gini','entropy'],\n",
    "                 max_depth=range(2,30), max_features=range(2,5),\n",
    "                 max_leaf_nodes=range(5,35), min_samples_leaf=[0.05])\n",
    "\n",
    "# Crear el objeto Random Forest para clasificación\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "#Autotuning de los modelos\n",
    "grid_rf = GridSearchCV(cv=5, estimator=rf_clf, n_jobs=-1, \n",
    "                     param_grid=param_dict_rf ,scoring='roc_auc')\n",
    "grid_rf.fit(X_train,y_train)\n",
    "rf_clf = grid_rf.best_estimator_\n",
    "\n",
    "# Entrenar el Random Forest\n",
    "rf_clf = rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye un modelo de Gradient Boosting usando Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el modelo\n",
    "param_dict_gb= dict(criterion=['friedman_mse', 'mse', 'mae'], \n",
    "                 loss=['deviance','exponential'], learning_rate=np.arange(0.01,0.1,0.01)\n",
    "                 max_depth=range(2,10), max_features=range(2,5),\n",
    "                 max_leaf_nodes=range(5,35), min_samples_leaf=[0.05])\n",
    "\n",
    "# Crear el objeto Gradient Boosting para clasificación\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "#Autotuning de los modelos\n",
    "grid_gb = GridSearchCV(cv=5, estimator=gb_clf, n_jobs=-1, \n",
    "                     param_grid=param_dict_gb ,scoring='roc_auc')\n",
    "grid_gb.fit(X_train,y_train)\n",
    "gb_clf = grid_gb.best_estimator_\n",
    "\n",
    "# Entrenar el Gradient Boosting\n",
    "gb_clf = gb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticos de precisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra la curva ROC de cada uno de los modelos junto con estadístico AUC que indica el área debajo de la curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelos = [{'label':'Decision Tree','modelo':tr_clf,},\n",
    "           {'label':'Random Forest','modelo':rf_clf,},\n",
    "           {'label':'Gradient Boosting','modelo':gb_clf,}]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "for m in modelos:\n",
    "    modelo = m['modelo']\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, modelo.predict_proba(X_test)[:,1])\n",
    "    auc = roc_auc_score(y_test, modelo.predict(X_test))\n",
    "    plt.plot(fpr,tpr,label='%s ROC (auc= %0.2f)' % (m['label'], auc))\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05]) \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (using validation data)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de un modelo se calcula comparando los valores reales del conjunto de prueba y los valores predichos por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in modelos:\n",
    "    modelo = m['modelo']\n",
    "    acc= accuracy_score(y_test,modelo.predict(X_test))\n",
    "    print('%s Accuracy: %0.2f' % (m['label'], acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
